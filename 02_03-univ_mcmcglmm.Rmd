## MCMCglmm

### Running the model

First load MCMCglmm:
```{r}
library(MCMCglmm)
```

The first model we will fit is a simple animal model with no fixed effects, and only an ‘animal’ random effect relating individuals to their additive genetic values through the pedigree. First we are going to define priors. In a way we might want to avoid using priors, because we would like all of the information in our analysis to come from our data.<!-- I dont understand this sentence -->
BUT by default MCMCglmm uses improper priors, but this can cause inferential and numerical problems. We will specify priors for the animal effect and the residual variance using the following code:

```{r}
prior1.1 <- list(
  G = list(G1 = list(V = 1, nu = 0.002)),
  R = list(V = 1, nu = 0.002)
)
```
A prior allowed the model to fit different variance structures. With the unique random effect "animal", we partitionned the phenotypic variance into two distinct variances matrices `G` (addtive genetic) and `R` (residual).
This prior specification is the simplistic one and often used because it was believed to be relatively uninformative, and is equivalent to an inverse-gamma prior with shape and scale equal to 0.001. In many cases it is relatively uninformative but when the posterior distribution for the variances has suport close to zero it can behave poorly. Parameter expanded priors (See Chapter 8 of the CourseNotes <!-- are you associated your class to this guide? -->) are gaining in popularity due to their better behaviour but for the purposes of this tutorial we will stick with the inverse-gamma prior. 
We have told MCMCglmm to pay little heed to our prior expectaion (V) by specifying a small degree of belief parameter (nu) of 0.002. Since this is a univariate analysis, the priors are matrix of order 1 and thus nu>0 is the smallest degree of belief that provides what is known as a ‘proper’ prior, avoiding numerical problems. In fact, there is a lot of information in the data regarding the marginal distributions of the parameters, and MCMCglmm will run most of the models that we suggest in these tutorials without priors. However, this is poor practice <!-- haha c'est ce que je fait tt le temps ..... -->, but we will therefore use this simple priors throughout these tutorials. We can now fit an animal model. The model to decompose variation in birth weight into genetic and residual effects is as follows:


The lower case “animal” is a can be a **special** word for MCMCglmm. If a `pedigree` argument is provided then `MCMCglmm` will recognize the term `animal` as the term to use to estimate additive genetic variance. When the argument `pedigree` is not provided then the word `animal` is not different than any other variable. However, instead of providing a pedigree argument to the call to MCMCglmm function it is much more fexible to use the `ginv` argument to specify the random effect that must be linked to the pedigree (with the inverse relatedness matrix). We thus first estimate thh inverse relatedness matrix using `inverseA()` then fit the animal model.
<!-- the variable names in gryphon and gryphonped are all in upper case, so the model need everything in upper case to run -->

```{r, cache=TRUE}
Ainv <- inverseA(gryphonped)$Ainv
model1.1 <- MCMCglmm(BWT ~ 1, random = ~ANIMAL, ginv = list(ANIMAL = Ainv), data = gryphon, prior = prior1.1)
```

After typing this code, MCMCglmm will run, taking about 20 seconds on a modern desk- top computer. The progress of the run will be printed to the screen. Also, note the warning message will be printed at the end of the run. This is natural too. In order for the MCMC algorithm to work, MCMCglmm must keep track of effects associated with unmeasured individuals appearing in the pedigree. This will not affect the answers, but when many unmeasured individuals exist, it can hinder the ability of the algorithm to explore the parameter space (more on this, and a solution, later). Lets have a look at the MCMCglmm outputs. First we will evaluate how confident we can be that MCMCglmm found good answers. By entering

```{r, fig.cap = "The posterior distribution of the fixed effect (the intercept, or mean) in model 1.1"}
plot(model1.1$Sol)
```

in the console, we get Figure 1 (p. 5). The plot on the left shows a time series of the values of 1000 samples of the posterior distribution of the the model intercept (mean birthweight). The plot on the right shows the same data as a distribution. Complicated statistical methods for estimating population means are of course of little interest; rather, we are examining these outputs to check that MCMCglmm’s algorithms worked well for our data and for this model. The important point here is that a consistent amount of variation around a largely unchanging mean value of the intercept was obtained (which give this fluctuating trace concentrated around the mean), and the posterior distribution of the intercept appears to be valid. More rigorous means of evaluation the independence of the samples in the posterior distribution (evaluating autocorrelation) are discussed in the MCMCglmm CourseNotes, available from CRAN. Note that your output for model 1.1 may not be identical to this due to Monte Carlo (random number) error. So everytimes, you run the model, you will get similar but slightly different results. 

The posterior distributions of the the variance components are generally of more inter- est <!-- ?? -->to animal model users. We can view plots of the posterior distribution for the variance components for model 1.1 by

```{r, fig.cap = "The posterior distributions of the variance components of model 1.1, based on an analysis with the default values for nitt, burnin, and thin in MCMCglmm"}
plot(model1.1$VCV)
```

which generates Figure 2 (p. 6). Here we see distributions of the estimates of the additive genetic (animal) and residual (units) effects. These samples contain some au- tocorrelation, i.e., trends are apparent in the left-hand plot. We can deal with this easily.

We will simply re-run the model for a longer number of iterations, and sample the chain less frequently. So far we have been running MCMCglmm with its default values. These defaults are a total run length of 13000 iterations, the first 3000 of which are discarded as a ‘burn-in’ period to make sure that the converges to the part of the parameter space where the maximum likelihood exists. The remaining 10000 iterations are sampled (estimates retained) every 10 iterations (the thinning interval). Because the values in the left-hand plots in figure 2 to appear to have different values at the beginning of the run, we might suspect that a longer burn-in period might be required. We can reduce the autocorrelation by lengthening the rest of the run and sampling the chain less frequently. The following code runs the same model 1.1, but is likely to produce better samples of the posterior distributions. This model should take about two minutes to analyze.

```{r, cache=TRUE}
model1.1 <- MCMCglmm(BWT ~ 1, random = ~ANIMAL, ginv = list(ANIMAL = Ainv), data = gryphon, nitt = 65000, thin = 50, burnin = 15000, prior = prior1.1, verbose = FALSE)
```

Notice that we have now included the command verbose=FALSE in the MCMCglmm call. We will continue this throughout the tutorial so that more complete screen outputs can be included in this document without using too much space. Now produce the plots of the samples of the fixed and random effects (they have not been included in this document). Note that the autocorrelation is much reduced. A more compact way to evaluate the validity of the posterior distributions is to calculate autocorrelation among samples, as follows:

```{r}
autocorr.diag(model1.1$VCV)
```

We will consider these levels of autocorrelation acceptable, at least for the purposes of this tutorial. Ideally, all samples of the posterior distribution should be independent, and the autocorrelation for all lag values greater than zero should be near zero. However, in practice this will not strictly be achievable for all analytical scenarios. Certainly the levels of autocorrelation observed here should not be tollerated in any formal analysis. Note that the validity of posterior distributions of any analysis should always be checked; however, for brevity we will not continue to be so consistently diligent throughout the rest of these tutorials. We can now proceed with confidence to recover some more information from these samples. We can obtain estimates of the additive genetic and residual variance by calculating the modes of the posterior distributions:

```{r}
posterior.mode(model1.1$VCV)
```

We can obtain the Bayesian equivalent of confidence intervals by calculating the the values of the estimates that bound 95% (or any other proportion) of the posterior distri- butions:

```{r}
HPDinterval(model1.1$VCV)
```

We specified weak priors in this analyses. Now we will check whether or not proper priors would have influenced the results that we obtained. The simplest way to do this is to rerun the model with different priors. Here we construct priors with a larger degree of belief parameter (`nu`), and we will specify that a large proportion of the variation is under genetic control (`V`):

```{r, cache=TRUE}
p.var <- var(gryphon$BWT, na.rm = TRUE)
prior1.1.2 <- list(G = list(G1 = list( V = matrix(p.var * 0.05), nu = 1)), 
                    R = list(V = matrix(p.var * 0.95), nu = 1))

model1.1.2 <- MCMCglmm(BWT ~ 1,
  random = ~ANIMAL, ginv = list(ANIMAL = Ainv),
  data = gryphon, prior = prior1.1.2, nitt = 65000, thin = 50,
  burnin = 15000, verbose = FALSE
)
posterior.mode(model1.1$VCV)
posterior.mode(model1.1.2$VCV)
```

and we can therefore conclude that the difference in the priors has little effect on the outcome of the analysis. This is typical for an analysis where lots of data are available relative to the complexity of the model, but is often not the case. In all cases, it is important to check the effect of priors on conclusions drawn from a model.
In addition, you can also specify the prior with previous knowledges or expectations of the variance, which would help the model to run faster.

### Estimating heritability
A useful property of Bayesian posterior distributions is that we can apply almost any transformation to these distributions and they will remain valid. This applies to the calculation of heritabilities. We can obtain an estimate of the heritability by applying the basic formula h 2 =V A /V P to each sample of the posterior disribution:

```{r}
posterior.heritability1.1 <- model1.1$VCV[, "ANIMAL"] / (model1.1$VCV[, "ANIMAL"] + model1.1$VCV[, "units"])
HPDinterval(posterior.heritability1.1, 0.95)
posterior.mode(posterior.heritability1.1)
```

Generate a plot of the posterior distribution of this heritability estimate:
```{r, fig.cap = "The posterior distributions the heritability from model 1.1"}
plot(posterior.heritability1.1)
```


### Adding fixed effects

To add effects to a univariate model we simply modify the fixed effect portion of the model specification:

```{r, cache=TRUE}
model1.2 <- MCMCglmm(BWT ~ SEX, random = ~ANIMAL, ginv = list(ANIMAL = Ainv), data = gryphon, prior = prior1.1, nitt = 65000, thin = 50, burnin = 15000, verbose = FALSE)
```

We can assess the significance of `SEX` as a fixed effect by examining its posterior distribution.

```{r}
posterior.mode(model1.2$Sol[, "SEX2"])

HPDinterval(model1.2$Sol[, "SEX2"], 0.95)
```

The posterior distribution of the `SEX2` term does not overlap zero. Thus we can infer that sex has a statistical effect on birthweight in this model and is a useful addition to the model, for most purposes. MCMCglmm has designated `SEX2` as the contrast between the two factor levels (male and female). It is also worth noting that the variance components have changed slightly:

```{r}
posterior.mode(model1.2$VCV)
```

In fact since sex effects were previously contributing to the residual variance of the model our estimate of V R (denoted ’units’ in the output) is now slightly lower than before. This has an important consequence for estimating heritability since if we calculate V P as V A +V R then as we include fixed effects we will soak up more residual variance driving V P . Assuming that V A is more or less unaffected by the fixed effects fitted then as V P goes down we expect our estimate of h 2 will go up.

```{r}
posterior.heritability1.2 <- model1.2$VCV[, "ANIMAL"] / (model1.2$VCV[, "ANIMAL"] + model1.2$VCV[, "units"])
posterior.mode(posterior.heritability1.2)
HPDinterval(posterior.heritability1.2, 0.95)
```

Here h 2 has increased slightly from 0.4829 to 0.5079 (again, your values may differ slightly due to Monte Carlo error). Which is the better estimate? It depends on what your question is. The first is an estimate of the proportion of variance in birth weight explained by additive effects, the latter is an estimate of the proportion of variance in birth weight after conditioning on sex that is explained by additive effects.


### Adding random effects

This is done by simply modifying the model statement in the same way, but requires addition of a prior for the new random effect. For instance, we can fit an effect of birth year:

```{r, cache=TRUE}
prior1.3 <- list(G = list(G1 = list(V = 1, nu = 0.002), G2 = list(V = 1, nu = 0.002)), R = list(V = 1, nu = 0.002))
model1.3 <- MCMCglmm(BWT ~ SEX, random = ~ ANIMAL + BYEAR, ginv = list(ANIMAL = Ainv), data = gryphon, nitt = 65000, thin = 50, burnin = 15000, prior = prior1.3, verbose = FALSE)
posterior.mode(model1.3$VCV)
```

Here the variance in birth weight explained by birth year is 0.7887. Note that although V A has changed somewhat, most of what is now partitioned as a birth year effect was previously partitioned as V R . Thus what we have really done here is to partition environ- mental effects into those arising from year to year differences versus everything else, and we do not really expect much change in h 2 (since now h 2 = V A /(V A + V BY + V R )). However, we get a somewhat different result if we also add a random effect of MOTHER to test for maternal effects:

```{r cache=TRUE}
p.var <- var(gryphon$bwt, na.rm = TRUE)
prior1.4 <- list(G = list(G1 = list(V = 1, nu = 0.002), G2 = list( V = 1, nu = 0.002), G3 = list(V = 1, nu = 0.002)), 
R = list( V = 1, nu = 0.002))
model1.4 <- MCMCglmm(BWT ~ SEX,
  random = ~ ANIMAL + BYEAR + MOTHER,
  ginv = list(ANIMAL = Ainv), data = gryphon, nitt = 65000, thin = 50, burnin = 15000,
  prior = prior1.4, verbose = FALSE
)
posterior.mode(model1.4$VCV)
```

Here partitioning of significant maternal variance has resulted in a further decrease in V R but also a decrease in V A . The latter is because maternal effects of the sort we simulated (fixed differences between mothers) will have the consequence of increasing similarity among maternal siblings. Consequently they can look very much like additive genetic effects and if present, but unmodelled, represent a type of ‘common environment effect’ that can - and will- cause upward bias in V A and so h 2 . Let’s compare the estimates of heritability from each of models 1.2, 1.3 and 1.4:

```{r}
posterior.heritability1.3 <- model1.3$VCV[, "ANIMAL"] / (model1.3$VCV[, "ANIMAL"] + model1.3$VCV[, "BYEAR"] + model1.3$VCV[, "units"])
posterior.heritability1.4 <- model1.4$VCV[, "ANIMAL"] / (model1.4$VCV[, "ANIMAL"] + model1.4$VCV[, "BYEAR"] + model1.4$VCV[, "MOTHER"] + model1.4$VCV[, "units"])
posterior.mode(posterior.heritability1.2)
posterior.mode(posterior.heritability1.3)
posterior.mode(posterior.heritability1.4)
```


### Testing significance of variance components

While testing the significance of fixed effects by evaluating whether or not their posterior distributions overlap zero was simple and valid, this approach does not work for variance components. Variance components are bound to be positive (given a proper prior), and thus even when a random effect is not meaningful, its posterior distribution will never overlap zero. Model comparisons can be performed using the deviance information criterion (DIC), although it should be noted that the properties of DIC are not well understood and that the DIC may be focused at the wrong level for most people’s intended level of inference - particularly with non-Gaussian responses. The implementation of DIC in MCMCglmm is further described in the reference manual. DIC values are calculated by MCMCglmm by default. Briefly, DIC like other information criteria balance model fit and model complexity simultaneously, and small values of DIC are prefered. We can compare models 1.4 and 1.3, i.e., models with and without the mother term:

```{r}
model1.3$DIC
model1.4$DIC
```

model 1.4 has a much lower DIC value. Since the maternal effect term is the only difference between the models, we can consider the inclusion of this term statistically justifiable. We should note however that DIC has a large sampling variance and should probably only be calculated based on much longer MCMC runs.



### Futher partitionning variance

Depending of the research question and the presence of different group within the dataset, MCMCglmm allowed to partition the variance at different level. For the example, we can partition the additive genetic and residual variance between SEXE (male and female) to estimate the sex-specific heritability 

```{r cache=TRUE}
prior1.4.SEX <- list(G = list(G1 = list(V = diag(2), nu = 1.002), G2 = list(V = 1, nu = 0.002),G3 = list(V = 1, nu = 0.002)), 
                    R = list(V = diag(2), nu = 1.002))
 
model1.4.SEX <- MCMCglmm(BWT ~ SEX,
  random = ~ idh(SEX):ANIMAL + BYEAR + MOTHER,
  rcov=~idh(SEX):units,
  ginv = list(ANIMAL = Ainv), data = gryphon, nitt = 65000, thin = 50, burnin = 15000,
  prior = prior1.4.SEX, verbose = FALSE
)
posterior.mode(model1.4.SEX$VCV)

posterior.heritability1.4.FEM <- model1.4.SEX$VCV[, "SEX1.ANIMAL"] / (model1.4.SEX$VCV[, "SEX1.ANIMAL"] + model1.4.SEX$VCV[, "BYEAR"] + model1.4.SEX$VCV[, "MOTHER"] + model1.4.SEX$VCV[, "SEX1.units"])
posterior.heritability1.4.MAL <- model1.4.SEX$VCV[, "SEX2.ANIMAL"] / (model1.4.SEX$VCV[, "SEX2.ANIMAL"] + model1.4.SEX$VCV[, "BYEAR"] + model1.4.SEX$VCV[, "MOTHER"] + model1.4.SEX$VCV[, "SEX2.units"])

posterior.mode(posterior.heritability1.4.FEM)
HPDinterval(posterior.heritability1.4.FEM, 0.95)
posterior.mode(posterior.heritability1.4.MAL)
HPDinterval(posterior.heritability1.4.MAL, 0.95)
```
Here, we can estimate the heritabilite for each sex. Both doesn'toverlap with zero, so we can conclude both sexes have significant heritability. However their CIs overlapp, so the heritability is not significantly different between sexes. An important quote to remember is "A difference in significance is not a significant difference" 



<!-- Est ce qu'il faut expliquer cette argument ds le guide pr=TRUE pour conserver les posterior des random effects? Pour faire des graphiques -->
