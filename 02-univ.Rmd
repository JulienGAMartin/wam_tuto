# Univariate animal model

This tutorial will demonstrate how to run a univariate animal model to estimate genetic variance in birth weight in the mighty gryphons.

## Scenario and data

### Scenario
In a population of gryphons there is strong positive selection on birth weight with heavier born individuals having, on average higher fitness. To find out whether increased birth weight will evolve in response to the selection, and if so how quickly, we want to estimate the heritability of birth weight.

### Data files
Open `gryphonped.csv` and `gryphon.csv` in your text editor. The structure and contents of these files is fairly self-explanatory. The pedigree file `gryphonped.csv` contains three columns containing unique IDs that correspond to each animal, its father, and its mother. Note that this is a multigenerational pedigree, with the earliest generation (for which parentage information is necessarily missing) at the beginning of the file. For later-born individuals maternal identities are all known but paternity information is incomplete (a common situation in real world applications).
The phenotype data, as well as additional factors and covariates that we may wish to include in our model are contained in `gryphon.csv`. Columns correspond to individual identity (`animal`), maternal identity (`mother`), year of birth (`byear`), sex (`sex`, where `1` is female and `2` is male), birth weight (`bwt`), and tarsus length (`tarsus`). Each row of the data file contains a record for a different offspring individual. Note that all individuals included in the data file must be included as offspring in the pedigree file.

We can read teh data file, using `read.csv()` which consider by default that `NA` is the symbol for missing values and that the first line of the file contains the column headers.

```{r, echo=FALSE}
gryphon <- read.csv("data/gryphon.csv")
```


It is a good idea to make sure that all variables are correctly assigned as numeric or factors:
```{r}
gryphon$animal <- as.factor(gryphon$animal)
gryphon$mother <- as.factor(gryphon$mother)
gryphon$byear <- as.factor(gryphon$byear)
gryphon$sex <- as.factor(gryphon$sex)
gryphon$bwt <- as.numeric(gryphon$bwt)
gryphon$tarsus <- as.numeric(gryphon$tarsus)
```

Similarly we can read in the pedigree file, using `read.csv()` which consider by default that `NA` is the symbol for missing values and that the first line of the file contains the column headers.

```{r, echo=FALSE}
gryphonped <- read.csv("data/gryphonped.csv")
str(gryphonped)
```

```{r}
gryphonped$id <- as.factor(gryphonped$id)
gryphonped$father <- as.factor(gryphonped$father)
gryphonped$mother <- as.factor(gryphonped$mother)
```

Now that we have imported the data and the pedigree file, we are ready to fit an animal model.

## Asreml-R

### Running the model

First we need to load the `asreml` library:
```{r}
library(asreml)
```

To be able to fit an animal model Asreml-r needs (the inverse of) the relationship matrix:

```{r}
ainv <- ainverse(gryphonped)
```

We are now ready to specify our first model:

```{r}
model1 <- asreml(
  fixed = bwt ~ 1, random = ~ vm(animal, ainv),
  residual = ~ idv(units),
  data = gryphon,
  na.action = na.method(x = "omit", y = "omit")
)
```

In this model, `bwt` is the response variable and the only fixed effect is the mean (the intercept, denoted as `1`). The only random effect we have fitted is `animal`, which will provide an estimate of $V_A$. Our random `animal` effect is connected to the inverse related matrix `ainv`.  `data=` specifies the name of the dataframe that contains our variables. Finally, we tell `asreml()` what to when it encounters `NA`s in either the dependent or predictor variables (in this case we choose to remove the records).

A note of the specification of the structure of the residuals: This simple univariate model will run fine without `residual=~idv(units)`. However, if you are going to use `vpredict()` to calculate the heritability (see below), not specifying the residuals in this way will result in a standard error for the heritability that is incorrect.

To see the estimates for the variance components, we run:

```{r}
summary(model1)$varcomp
```

We fitted a single random effect so have partitioned the phenotypic variance into two components. The `vm(animal, ainv)` variance component is $V_A$ and is estimated as `r round(summary(model1)$varcomp[1,1], 2)`. Given that the ratio of $V_A$ to its standard error (`z.ratio`) is considerably larger than 2 (_i.e._ the parameter estimate is more than 2 SEs from zero) this looks likely to be highly significant. The `units!units` component refers to the residual variance $V_R$, and `units$R` should be ignored. If you don't include `residual=~idv(units)`in your model specification, `units$R` will provide you with the residual variance.

### Estimating heritability

We can calculate the $h^2$ of birth weight from the components above since $h^2 = V_A/V_P = V_A/(V_A+V_R)$. Thus according to this model, $h^2$ = `r round(summary(model1)$varcomp[1,1],2)` / (`r round(summary(model1)$varcomp[1,1], 2)` + `r round(summary(model1)$varcomp[2,1], 2)`) = `r round(summary(model1)$varcomp[1,1] / (summary(model1)$varcomp[1,1] +  summary(model1)$varcomp[2,1]),2)`.

Alternatively we can use the `vpredict()` function to calculate $h^2$ and its standard error:
```{r}
vpredict(model1, h2.bwt ~ V1 / (V1 + V2))
```

### Adding fixed effects

To add fixed effects to a univariate model simply modify the model statement. For example we might know (or suspect) that birth weight is a sexually dimorphic trait and therefore fit a model
```{r}
model2 <- asreml(
  fixed = bwt ~ 1 + sex,
  random = ~ vm(animal, ainv),
  residual = ~ idv(units),
  data = gryphon,
  na.action = na.method(x = "omit", y = "omit")
)
```

Now we can look at the fixed effects parameters and assess their significance with a conditional Wald F-test:
```{r, eval=FALSE}
summary(model2, coef = TRUE)$coef.fixed
wald.asreml(model2, ssType = "conditional", denDF = "numeric")
```

```{r, echo=FALSE}
summary(model2, coef = TRUE)$coef.fixed
w <- wald.asreml(model2, ssType = "conditional", denDF = "numeric")
w$Wald[1:6]
```

The very small probability (`Pr`) in the Wald test above shows that `sex` is a highly significant fixed effect, and from the parameter estimates we can see that the average male (sex 2) is 2.2 kg ($\pm$ 0.16 SE) heavier than the average female (sex 1). However, when we look at the variance components in the model including `sex` as a fixed effect, we see that they have changed slightly from the previous model:
```{r}
summary(model2)$varcomp
```
In fact since `sex` effects were previously contributing to the residual variance of the model, our estimate of $V_R$ (denoted `units!R` in the output) is now slightly lower than before. This has an important consequence for estimating heritability since if we calculate $V_P$ as $V_A$+$V_R$ then as we include fixed effects we will soak up more residual variance driving $V_P$. Assuming that $V_A$ is more or less unaffected by the fixed effects fitted then as $V_P$ goes down we expect our estimate of $h^2$ will go up:

```{r}
vpredict(model2, h2.bwt ~ V1 / (V1 + V2))
```

```{r, echo=FALSE}
h2.1 <- vpredict(model1, h2.bwt ~ V1 / (V1 + V2))
h2.2 <- vpredict(model2, h2.bwt ~ V1 / (V1 + V2))
```

Here $h^2$ has increased slightly from `r round(h2.1[1],2)` to `r round(h2.2[1],2)`. Which is the better estimate? It depends on what your question is. The first is an estimate of the proportion of variance in birth weight explained by additive effects, the latter is an estimate of the proportion of variance in birth weight _after conditioning on sex_ that is explained by additive effects.

### Adding random effects
This is done by simply modifying the model statement in the same way. For instance fitting

```{r}
model3 <- asreml(
  fixed = bwt ~ 1 + sex,
  random = ~ vm(animal, ainv) + byear,
  residual = ~ idv(units),
  data = gryphon,
  na.action = na.method(x = "omit", y = "omit")
)
```

results in an additional variance component of birth year:

```{r}
summary(model3)$varcomp
```

Here the variance in `bwt` explained by `byear` is 0.886 and, based on the `z.ratio`, appears to be significant. Thus we would conclude that year-to-year variation (_e.g._, in weather, resource abundance) contributes to $V_P$. Note that although $V_A$ has changed somewhat, as most of what is now partitioned as a birth year effect was previously partitioned as $V_R$. Thus what we have really done here is to partition environmental effects into those arising from year-to-year differences versus everything else, and we do not really expect much change in $h^2$ (since now $h^2 = V_A/ (V_A+V_{BY}+V_R)$).

However, we get a somewhat different result if we also add a random effect of `mother` to test for maternal effects:

```{r}
model4 <- asreml(
  fixed = bwt ~ 1 + sex,
  random = ~ vm(animal, ainv) + byear + mother,
  residual = ~ idv(units),
  data = gryphon,
  na.action = na.method(x = "omit", y = "omit")
)
```

Gives estimated variance components of

```{r}
summary(model4)$varcomp
```

Here partitioning of significant maternal variance has resulted in a further decrease in $V_R$ but also a decrease in $V_A$. The latter is because maternal effects of the sort we simulated (fixed differences between mothers) will have the consequence of increasing similarity among maternal siblings. Consequently they can look very much like additive genetic effects and if present, but unmodelled, represent a type of "common environment effect"" that can - and will - cause upward bias in $V_A$ and so $h^2$.

### Testing significance of random effects

A final point to note in this tutorial is that while the `z.ratio` (`component`/`std.error`) reported is a good indicator of likely statistical significance (>1.96?), the standard errors are approximate and are not recommended for formal hypothesis testing. A better approach is to use likelihood-ratio tests.

For example, to test the significance of maternal effects we could compare models with and without the inclusion of maternal identity as a random effect and compare the final log-likelihoods of these models.

```{r}
model4$loglik
```

shows that the model including maternal identity has a log-likelihood of `r round(model4$loglik, 3)`, and

```{r}
model3$loglik
```

shows that the model excluding maternal identity has a log-likelihood of `r round(model3$loglik, 3)`.

A test statistic equal to twice the absolute difference in these log-likelihoods is assumed to be distributed as Chi square with one degree of freedom. In this case we would conclude that the maternal effects are highly significant since:
2 $\times$ (`r model4$loglik` - `r model3$loglik`) equals `r 2*( model4$loglik - model3$loglik)`, and the p-value that comes with this is:

```{r}
1 - pchisq(2 * (model4$loglik - model3$loglik), 1)
```

As P < 0.0001 we would therefore conclude that the additional of maternal identity as a random effect significantly improves the model, given an increase in log-likelihood of approximately `r round(model4$loglik - model3$loglik, 0)`.


<!--
################################################################################
################################################################################
################################################################################
-->

## gremlin

TODO (maybe just bother Matthew to do it)

<!--
################################################################################
################################################################################
################################################################################
-->


## MCMCglmm

### Running the model

First load MCMCglmm:
```{r}
library(MCMCglmm)
```

The first model we will fit is a simple animal model with no fixed effects, and only an ‘animal’ random effect relating individuals to their additive genetic values through the pedigree. First we are going to define priors. In a way we might want to avoid using priors, because we would like all of the infromation in our analysis to come from our data. By default MCMCglmm uses improper priors, but this can cause inferential and numerical problems. We will specify priors for the animal effect and the residual variance using the following code:

```{r}
Ainv <- inverseA(gryphonped)$Ainv
prior1.1 <- list(
  G = list(G1 = list(V = 1, nu = 0.002)),
  R = list(V = 1, nu = 0.002)
)
```

This prior specification used to be used a lot because it was believed to be relatively uninformative, and is equivalent to an inverse-gamma prior with shape and scale equal to 0.001. In many cases it is relatively uninformative but when the posterior distribution for the variances has suport close to zero it can behave poorly. Parameter expanded priors (See Chapter 8 of the CourseNotes) are gaining in popularity due to their better behaviour but for the purposes of this tutorial we will stick with the inverse-gamma prior. We have told MCMCglmm to pay little heed to our prior expectaion (V) by specifying a small degree of belief parameter (nu) of 0.002. Since this is a univariate analysis, the priors are matricies of order 1 and thus nu>0 is the smallest degree of belief that provides what is known as a ‘proper’ prior, avoiding numerical problems. In fact, there is a lot of information in the data regarding the marginal distributions of the parameters, and MCMCglmm will run most of the models that we suggest in these tutorials without priors. However, this is poor practice, and we will therefore use priors throughout these tutorials. We can now fit an animal model. The model to decompose variation in birth weight into genetic and residual effects is as follows:

```{r, cache=TRUE}
model1.1 <- MCMCglmm(bwt ~ 1, random = ~animal, ginv = list(animal = Ainv), data = gryphon, prior = prior1.1)
```

After typing this code, MCMCglmm will run, taking about 20 seconds on a modern desk- top computer. The progress of the run will be printed to the screen. Also, note the warning message will be printed at the end of the run. This is natural too. In order for the MCMC algorithm to work, MCMCglmm must keep track of effects associated with unmeasured individuals appearing in the pedigree. This will not affect the answers, but when many unmeasured individuals exist, it can hinder the ability of the algorithm to explore the parameter space (more on this, and a solution, later). Lets have a look at the MCMCglmm outputs. First we will evaluate how confident we can be that MCMCglmm found good answers. By entering

```{r, fig.cap = "The posterior distribution of the fixed effect (the intercept, or mean) in model 1.1"}
plot(model1.1$Sol)
```

in the console, we get Figure 1 (p. 5). The plot on the left shows a time series of the values of 1000 samples of the posterior distribution of the the model intercept (mean birthweight). The plot on the right shows the same data as a distribution. Complicated statistical methods for estimating population means are of course of little interest; rather, we are examining these outputs to check that MCMCglmm’s algorithms worked well for our data and for this model. The important point here is that a consistent amount of variation around a largely unchanging mean value of the intercept was obtained, and the posterior distribution of the intercept appears to be valid. More rigorous means of evaluation the independence of the samples in the posterior distribution (evaluating autocorrelation) are discussed in the MCMCglmm CourseNotes, available from CRAN. Note that your output for model 1.1 may not be identical to this due to Monte Carlo (random number) error.

The posterior distributions of the the variance components are generally of more inter- est to animal model users. We can view plots of the posterior distribution for the variance components for model 1.1 by

```{r, fig.cap = "The posterior distributions of the variance components of model 1.1, based on an analysis with the default values for nitt, burnin, and thin in MCMCglmm"}
plot(model1.1$VCV)
```

which generates Figure 2 (p. 6). Here we see distributions of the estimates of the additive genetic (animal) and residual (units) effects. These samples contain some au- tocorrelation, i.e., trends are apparent in the left-hand plot. We can deal with this easily.

We will simply re-run the model for a longer number of iterations, and sample the chain less frequently. So far we have been running MCMCglmm with its default values. These defaults are a total run length of 13000 iterations, the first 3000 of which are discarded as a ‘burn-in’ period to make sure that the converges to the part of the parameter space where the maximum likelihood exists. The remaining 10000 iterations are sampled (es- timates retained) every 10 iterations (the thinning interval). Because the values in the left-hand plots in figure 2 to appear to have different values at the beginning of the run, we might suspect that a longer burn-in period might be required. We can reduce the autocorrelation by lengthening the rest of the run and sampling the chain less frequently. The following code runs the same model 1.1, but is likely to produce better samples of the posterior distributions. This model should take about two minutes to analyze.

```{r, cache=TRUE}
model1.1 <- MCMCglmm(bwt ~ 1, random = ~animal, ginv = list(animal = Ainv), data = gryphon, nitt = 65000, thin = 50, burnin = 15000, prior = prior1.1, verbose = FALSE)
```

Notice that we have now included the command verbose=FALSE in the MCMCglmm call. We will continue this throughout the tutorial so that more complete screen outputs can be included in this document without using too much space. Now produce the plots of the samples of the fixed and random effects (they have not been included in this document). Note that the autocorrelation is much reduced. A more compact way to evaluate the validity of the posterior distributions is to calculate autocorrelation among samples, as follows:

```{r}
autocorr.diag(model1.1$VCV)
```

We will consider these levels of autocorrelation acceptable, at least for the purposes of this tutorial. Ideally, all samples of the posterior distribution should be independent, and the autocorrelation for all lag values greater than zero should be near zero. However, in practice this will not strictly be achievable for all analytical scenarios. Certainly the levels of autocorrelation observed here should not be tollerated in any formal analysis. Note that the validity of posterior distributions of any analysis should always be checked; however, for brevity we will not continue to be so consistently diligent throughout the rest of these tutorials. We can now proceed with confidence to recover some more information from these samples. We can obtain estimates of the additive genetic and residual variance by calculating the modes of the posterior distributions:

```{r}
posterior.mode(model1.1$VCV)
```

We can obtain the Bayesian equivalent of confidence intervals by calculating the the values of the estimates that bound 95% (or any other proportion) of the posterior distri- butions:

```{r}
HPDinterval(model1.1$VCV)
```

We specified weak priors in this analyses. Now we will check whether or not proper priors would have influenced the results that we obtained. The simplest way to do this is to rerun the model with different priors. Here we construct priors with a larger degree of belief parameter, and we will specify that a large proportion of the variation is under genetic control:

```{r, cache=TRUE}
p.var <- var(gryphon$bwt, na.rm = TRUE)
prior1.1.2 <- list(G = list(G1 = list(
  V = matrix(p.var * 0.05),
  nu = 1
)), R = list(V = matrix(p.var * 0.95), nu = 1))
model1.1.2 <- MCMCglmm(bwt ~ 1,
  random = ~animal, ginv = list(animal = Ainv),
  data = gryphon, prior = prior1.1.2, nitt = 65000, thin = 50,
  burnin = 15000, verbose = FALSE
)
posterior.mode(model1.1$VCV)
posterior.mode(model1.1.2$VCV)
```

and we can therefore conclude that the difference in the priors has little effect on the outcome of the analysis. This is typical for an analysis where lots of data are available relative to the complexity of the model, but is often not the case. In all cases, it is important to check the effect of priors on conclusions drawn from a model.

### Estimating heritability
A useful property of Bayesian posterior distributions is that we can apply almost any transformation to these distributions and they will remain valid. This applies to the calculation of heritabilities. We can obtain an estimate of the heritability by applying the basic formula h 2 =V A /V P to each sample of the posterior disribution:

```{r}
posterior.heritability1.1 <- model1.1$VCV[, "animal"] / (model1.1$VCV[, "animal"] + model1.1$VCV[, "units"])
HPDinterval(posterior.heritability1.1, 0.95)
posterior.mode(posterior.heritability1.1)
```

Generate a plot of the posterior distribution of this heritability estimate:
```{r, fig.cap = "The posterior distributions the heritability from model 1.1"}
plot(posterior.heritability1.1)
```


### Adding fixed effects

To add effects to a univariate model we simply modify the fixed effect portion of the the model specification:

```{r, cache=TRUE}
model1.2 <- MCMCglmm(bwt ~ sex, random = ~animal, ginv = list(animal = Ainv), data = gryphon, prior = prior1.1, nitt = 65000, thin = 50, burnin = 15000, verbose = FALSE)
```

We can assess the significance of sex as a fixed effect by examining its posterior distribution.

```{r}
posterior.mode(model1.2$Sol[, "sex2"])

HPDinterval(model1.2$Sol[, "sex2"], 0.95)
```

The posterior distribution of the sex2 term does not overlap zero. Thus the we can infer that sex has a statistical effect on birthweight in this model and is a useful addition to the model, for most purposes. MCMCglmm has designated sex2 as the contrast between the two factor levels (male and female). It is also worth noting that the variance components have changed slightly:

```{r}
posterior.mode(model1.2$VCV)
```

In fact since sex effects were previously contributing to the residual variance of the model our estimate of V R (denoted ’units’ in the output) is now slightly lower than before. This has an important consequence for estimating heritability since if we calculate V P as V A +V R then as we include fixed effects we will soak up more residual variance driving V P . Assuming that V A is more or less unaffected by the fixed effects fitted then as V P goes down we expect our estimate of h 2 will go up.

```{r}
posterior.heritability1.2 <- model1.2$VCV[, "animal"] / (model1.2$VCV[, "animal"] + model1.2$VCV[, "units"])
posterior.mode(posterior.heritability1.2)
HPDinterval(posterior.heritability1.2, 0.95)
```

Here h 2 has increased slightly from 0.4829 to 0.5079 (again, your values may differ slightly due to Monte Carlo error). Which is the better estimate? It depends on what your question is. The first is an estimate of the proportion of variance in birth weight explained by additive effects, the latter is an estimate of the proportion of variance in birth weight after conditioning on sex that is explained by additive effects.


### Adding random effects

This is done by simply modifying the model statement in the same way, but requires addition of a prior for the new random effect. For instance, we can fit an effect of birth year:

```{r, cache=TRUE}
prior1.3 <- list(G = list(G1 = list(V = 1, nu = 0.002), G2 = list(V = 1, nu = 0.002)), R = list(V = 1, nu = 0.002))
model1.3 <- MCMCglmm(bwt ~ sex, random = ~ animal + byear, ginv = list(animal = Ainv), data = gryphon, nitt = 65000, thin = 50, burnin = 15000, prior = prior1.3, verbose = FALSE)
posterior.mode(model1.3$VCV)
```

Here the variance in birth weight explained by birth year is 0.7887. Note that although V A has changed somewhat, most of what is now partitioned as a birth year effect was previously partitioned as V R . Thus what we have really done here is to partition environ- mental effects into those arising from year to year differences versus everything else, and we do not really expect much change in h 2 (since now h 2 = V A /(V A + V BY + V R )). However, we get a somewhat different result if we also add a random effect of mother to test for maternal effects:

```{r cache=TRUE}
p.var <- var(gryphon$bwt, na.rm = TRUE)
prior1.4 <- list(G = list(G1 = list(V = 1, nu = 0.002), G2 = list(
  V = 1,
  nu = 0.002
), G3 = list(V = 1, nu = 0.002)), R = list(
  V = 1,
  nu = 0.002
))
model1.4 <- MCMCglmm(bwt ~ sex,
  random = ~ animal + byear + mother,
  ginv = list(animal = Ainv), data = gryphon, nitt = 65000, thin = 50, burnin = 15000,
  prior = prior1.4, verbose = FALSE
)
posterior.mode(model1.4$VCV)
```

Here partitioning of significant maternal variance has resulted in a further decrease in V R but also a decrease in V A . The latter is because maternal effects of the sort we simulated (fixed differences between mothers) will have the consequence of increasing similarity among maternal siblings. Consequently they can look very much like additive genetic effects and if present, but unmodelled, represent a type of ‘common environment effect’ that can - and will- cause upward bias in V A and so h 2 . Let’s compare the estimates of heritability from each of models 1.2, 1.3 and 1.4:

```{r}
posterior.heritability1.3 <- model1.3$VCV[, "animal"] / (model1.3$VCV[, "animal"] + model1.3$VCV[, "byear"] + model1.3$VCV[, "units"])
posterior.heritability1.4 <- model1.4$VCV[, "animal"] / (model1.4$VCV[, "animal"] + model1.4$VCV[, "byear"] + model1.4$VCV[, "mother"] + model1.4$VCV[, "units"])
posterior.mode(posterior.heritability1.2)
posterior.mode(posterior.heritability1.3)
posterior.mode(posterior.heritability1.4)
```


### Testing significance of variance components

While testing the significance of fixed effects by evaluating whether or not their posterior distributions overlap zero was simple and valid, this approach does not work for vari- ance components. Variance components are bound to be positive (given a proper prior), and thus even when a random effect is not meaningful, its posterior distribution will never overlap zero. Model comparisons can be performed using the deviance information criterion (DIC), although it should be noted that the properties of DIC are not well un- derstood and that the DIC may be focused at the wrong level for most people’s intended level of infernce - particularly with non-Gaussian responses. The implementation of DIC in MCMCglmm is further described in the reference manual. DIC values are calculated by MCMCglmm by default. Briefly, DIC like other information criteria balance model fit and model complexity simultaneously, and small values of DIC are prefered. We can compare models 1.4 and 1.3, i.e., models with and without the mother term:

```{r}
model1.3$DIC
model1.4$DIC
```

model 1.4 has a much lower DIC value. Since the maternal effect term is the only difference between the models, we can consider the inclusion of this term statistically justifiable. We should note however that DIC has a large sampling variance and should probably only be calculated based on much longer MCMC runs.


<!--
################################################################################
################################################################################
################################################################################
-->

## brms

```{r eval = params$fit_all}
library(brms)
Amat <- as.matrix(nadiv::makeA(gryphonped))
brms_m1.1 <- brm(
  bwt ~ 1 + (1 | gr(animal, cov = Amat)),
  data = gryphon,
  data2 = list(Amat = Amat),
  family = gaussian(),
  chains = 2, cores = 2, iter = 1000
)
save(brms_m1.1, file = "data/brms_m1_1.rda")
```

```{r}
load("data/brms_m1_1.rda")
summary(brms_m1.1)
plot(brms_m1.1)
```
